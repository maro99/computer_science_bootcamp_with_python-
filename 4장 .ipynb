{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "컴퓨터는 0과 1밖에 알지 못하는데 어떻게 문자를 표현할 수 있을까?\n",
    "'a'라는 문자 혹은 '가' 라는 문자를 메모리에 어떻게 저장할까? \n",
    "\n",
    "        4. 문자와 문자열\n",
    "            1. 아스키코드\n",
    "            2. 유니코드\n",
    "            3. 유니코드 인코딩 방식\n",
    "                3.1 UTF-8\n",
    "                3.2 UTF-16\n",
    "                3.3 UTF-32\n",
    "            4.파이선 문자열의 특징\n",
    "            5.마무리\n",
    "\n",
    "\n",
    "\n",
    "1. 아스키코드\n",
    "\n",
    "        용어 : \n",
    "                문자 인코딩( character encoding) : 문자 집합을 메모리에 저장하거나 통신하는 데 사용하기 위해 부호화하는 방식.\n",
    "                문자 집합 (character set) : 문자를 모아 둔 것.\n",
    "                코드 포인트(code point) : 컴퓨터에 문자 인식 위해 문자 하나에 정수 하나를 핑해 둠. 이렇게 매핑된 정수를 코드 포인트.\n",
    "                부호화된 문자 집합(Code Character Set, CCS) : 문자와 문자에 매핑된  코드 포인트를 모아 놓은 집합 \n",
    "                \n",
    "        아스키 코드에는 정수 0부터 127 까지 문자와 매핑되어 있다. (0000 0000 ~ 0111 1111) 7 비트로 표현 가능.\n",
    "        int 형, short형 쓰면 메모리가 낭비된다. 그래서 문자 표현 위해 char 라는 새로운 정수 자료형을 만들었다. \n",
    "\n",
    "2.  유니코드\n",
    "\n",
    "        인터넷이 보급 되며 세계 각국 사람 커뮤니 케이션 원활에 졌고 좀더 많은 언어가 필요해 졌다. \n",
    "        이때 나온 해결방법이 7비트로 표현한 문자를 16 비트로 확장하는것 \n",
    "        7비트 (128개 문자) ---> 16 비트 ( 65536개 문자) 로 표현 \n",
    "        여기에 더해 수 하나에 다시 문자 하나를 일대이로 대응한 새로운 표를 만들었는데 이 테이블이 바로 유니코드.\n",
    "        ( 기존 아스키 0x0000~ 0x007f는 그대로 사용)\n",
    "\n",
    "        언어 대부분이 포함된 첫번째 테이블이 다국어 평면 (BMP)라 놓고 비슷한것 16개 더 만듬.\n",
    "\n",
    "3. 유니코드 인코딩 방식 \n",
    "\n",
    "        용어 :\n",
    "                코드 유닛 ( code unit) : 코드 포인트를 특정한 방법으로 인코딩했을 때 변환되어 얻어지는 비트의 나열\n",
    "                문자 인코딩 방식(character encoding scheme, ces): 코드 유닛을 옥텟으로 나열하여 변환하는 방법.\n",
    "                                    옥텟: 데이터 단위 . 8비트. 현제 컴퓨터는 모두 8 비트 단위를 사용해서 코드 유닛을 옥텟으로 변환해도 실제비트 부동\n",
    "\n",
    "        유니 코드는 2바이트로 숫자 하나에 문자 하나 대응하여 문자를 표현한다. \n",
    "        기본 다국어 평면을 포함해 평면이 열일곱 개 있으므로 모든 문자 표현하려면 3바이트 필요하다. \n",
    "        코드 유닛 모두 3바이트 하고 코드 포인트 그대로 저장하면 될것 같지만 쉽게 결정하면 안된다. \n",
    "        만약 1바이트 정수만 저장할 수 있는 시스템이면 3바이트짜리 정수는 저장할 수 없어서 시스템이서 한글 표현 불가능.\n",
    "        \n",
    "        그러무로 다양한 코드 유닛을 갖는 인코딩 방식을 두어 유연하게 대쳐한다. \n",
    "        UTF-8, UTF-16. UTF - 32\n",
    "        ( 이하 설명은 책이 너무 간략해서 유튜브 찾아봤다. 맨아래 보충 자료 참고해라.)\n",
    "\n",
    "4. 파이선 문자열의 특징.\n",
    "        \n",
    "        c/c++ 에서 문자열을 변수로 만들면 요소인 문자를 변경할 수 있고 \n",
    "        문자열을 상수로 만들면 요소를 변경할 수 없다. 즉 프로그래머가 변경 가능성을 선택할 수 있다. \n",
    "        \n",
    "        하지만 파이썬의 문자열은 요소를 변경할 수 없다.\n",
    "\n",
    "            string = 'abcde'\n",
    "            string[2] = 'a' 와 같은 코드는 오류 생긴다. \n",
    "\n",
    "        위의 예에서 가운데 c를 x로 변경하고 싶다면 어떻게 해야 할까? \n",
    "        문자열 슬라이싱을 사용해 요소를 변경한다. \n",
    "        주의할 점은 string의 요소를 직접 변경하지 않는다는 것.\n",
    "\n",
    "            new_string = string[:2] + 'x' + string[3:]\n",
    "\n",
    "        혹은 built in function 중 replace사용 \n",
    "        \n",
    "            new_string = string.replace('c', 'x')\n",
    "        \n",
    "\n",
    "보충. \n",
    "\n",
    "https://youtu.be/B1Sf1IhA0j4\n",
    "https://youtu.be/-oYfv794R9s\n",
    "https://youtu.be/vLBtrd9Ar28\n",
    "\n",
    "다음 튜토리얼 1~ 3 참고.\n",
    "\n",
    "\n",
    "\n",
    "#1 ASCCII\n",
    "\n",
    "        encoding\n",
    "1 < -----------------   'A'\n",
    "    -------------------->\n",
    "        decoding\n",
    "\n",
    "\n",
    "1    :    'A'\n",
    "2    :    'B'\n",
    "3    :    'C'\n",
    "\n",
    "이런관계를 character mapping\n",
    "이런 것이 여러게 있는 table을 code page\n",
    "\n",
    "\n",
    "우리는 보통 byte단위를 컴퓨터에서 쓴다. \n",
    "\n",
    "ASCCI (America standard code for inteformation interchage)는 \n",
    "이  character mappint이 0~127 개 되어 있다. \n",
    "1byte = 8 bit중 7bit를 사용해서 ASCII를 정의한다. \n",
    "8번째는 free해서 이것을 정의해서 더많은 종류의 char을 mapping해서 사용했다. (영어 외의 언어, 수학 기호 등)\n",
    "\n",
    "서로다른 고립된 환경에서는 각각 필요한 부분을 8 비트상에서 커스텀해서 \n",
    "다르 encoding scheme을 가지고 사용해서 영향이 서로 없었는데 \n",
    "인터넷시대가 되면서 문제가 생겼다.--->서로 호환 안됨.\n",
    "\n",
    "이것을 해결하기 위해 unicode가 등장했다. \n",
    "\n",
    "\n",
    "#2 UNICODE\n",
    "\n",
    "서로다른 언어들이 서로 다른 encoding scheme이 아닌 \n",
    "같은 encoding scheme 전세계적으로 써서 표준화 시킨것.\n",
    "\n",
    "ASCII :    \n",
    "            A ---->0010 0001       integer 과  char 을 mapping하는데 1byte 썼다. \n",
    "\n",
    "\n",
    "UNICODE :\n",
    "                                                    code point                                     인코딩                          unicode \n",
    "            A -------------------------> 1000001  ----------------------------> utf-32 ----------------> [          ]\n",
    "                                                          65                                              utf-16\n",
    "                                                                                                             utf -8\n",
    "\n",
    "            code point라는 integer 단계 거친뒤 크기에 맞게, 의도에 맞게 utf인코딩 방식을 선택해서 거친뒤 unicode 방식의 데이터가 된다.\n",
    "            ( 문자열 ~ 정수  매핑을 실제 메모리상에 저장하는 표현과 분리하기 위함.)\n",
    "\n",
    "#3  UTF - 32 \n",
    "\n",
    "Integer -----------------------> [][][][][][]....... 32bit (4 byte)\n",
    "\n",
    "fixed width encoding scheme( 32자리 bit로 그 폭이 고정되있다)\n",
    "\n",
    "ex)\n",
    "\n",
    "code point가 \n",
    "                        codepoint                            인코딩 \n",
    "A -----------> 1000001 이라면 ----------> utf -32 -------> 000000....1000001 (32bit)\n",
    "\n",
    "이렇게 저장된다. 남은것은 다 zero  붙이고 \n",
    "오른쪽에 code point 붙여버림 .\n",
    "\n",
    "이것이  code point를  utf -32 encoding scheme에 맞춰서 메모리상에 표현한 것.\n",
    "\n",
    "(  영어권에서는 잘 사용하지 않는 방식이다.\n",
    "    모든 알파벳 1bytr로 표현한 ASCCI 로 표현가능 한데 4byte로 표현하니까 비 효율적... 메모리 낭비!!)\n",
    "---------> 2byte만으로도 표현 가능한 utf -16 등장.\n",
    "\n",
    "\n",
    "#4 UTF -16 \n",
    "    \n",
    "( 주의 해야할것.  \n",
    "utf-32, 16, 8은 자리수를 의미하는 것이 아니다.)\n",
    "\n",
    "utf 16에선 \n",
    "code point가 16자리(2byte) 혹은 필요하다면 32자리(4byte)\n",
    "(16bit  한덩어리 or 16bit 두덩어리 .     variable length character set 이라 한다.)\n",
    "메모리의 bit width 가 고정되어 있지 않다. \n",
    "\n",
    "utf -32 와 비교해서 장점은 \n",
    "ASCCII 코드를 인코딩할 경우 UTF-16의 경우 기존의 4배가 아니라 최소 2배이다. \n",
    "\n",
    "( 여전히 UTF -16 ----> ASCII 역으로 호환 불가능 하다. \n",
    "  즉 ASCII로 작성된 프로그램을 먼저 UTF-16으로 encoding한후 UTF-16형식으로 해독 가능하다. )\n",
    "\n",
    "utf -32 에서 예시로든 1000 001의 경우 32자리 4byte 필요 없다. \n",
    "                \n",
    "                     codepoint                         인코딩\n",
    "A----------> 1000 001   ---------------> utf - 32 ------------->    00000000     01000001    (  00 41)\n",
    "\n",
    "    첫번째 byte = 0x00 , 두번째 바이트는 codepoint내용그대로 우에 붙임 0x41\n",
    "\n",
    "                                                                                                                 \n",
    "Big endian? Little Endian? \n",
    "\n",
    "00 41      vs  41 00 두가지 포멧으로 utf -16으로 인코딩된 데이터를 나타낼 수 있다. \n",
    "\n",
    "후자인 41 00 이 합리적일 수 있는 이유는 ? \n",
    "---------->\n",
    "[][][][][][]\n",
    "6 byte size의 integer array가 있다고 해보자. \n",
    "운영체제제가 반환하는 주소는 첫 byte 이다.\n",
    "만약 변수를 int * ptr_one 이렇게 할당했다면 \n",
    "이 변수는 --->위 sequeence 의 첫 byte 가리킨다. \n",
    "그리고 다음의 integer는 이 메모리 address 를 증가시켜서 접근 가능하다. \n",
    "\n",
    "그래서 만약 데이커가 2 바이트에 저장되어 있다면?(우리의 경우)\n",
    "이것을 두개의 bytes로 배열에 저장해야 한다. \n",
    "00 41 이렇게 저장할 경우 \n",
    "00 확인 후 41 로 가지만 \n",
    "41 00 이렇게 저장시 바로 41부터 확인하고 끝낸다. \n",
    "그래서 41 / 00 이 더 효율적이다. \n",
    "\n",
    "00 41 처럼  작은 byte 먼저 나타내는 경우를  Big endian\n",
    "41 00 처럼 큰  byte를 먼저 나타내는 경우를   little endian 이라 한다. \n",
    "\n",
    "이것을 구분하기 위해서 nonchardata를  파일 앞에 추가한다. \n",
    "\n",
    "byte order mark( 255, 254)\n",
    "\n",
    "255 / 254 ( 0xFFFE)  ---->  little endian \n",
    "254 / 255 ( 0XFEFF) ---->  big endian \n",
    "\n",
    "\n",
    "여전히 아스키 쓰는 사람에거 1byte만 있어도 되는데 2byte나 써서 메모리 낭비하는 느낌이다. \n",
    "그래서 등장한 것이 utf -8\n",
    "\n",
    "\n",
    "#5  UTF -8 \n",
    "\n",
    "unicode <--------> ascci \n",
    "\n",
    "둘사이에 호환 밑 역호환이 가능하게 하고 싶다. \n",
    "또한 메모리 상의 낭비도 없에고 싶다. 그래서 생긴것이 utf -8 \n",
    "(ASSCII로 쓰여진 파일 을  ASCII, UTF-8로 둘다 해석 가능하고 싶다.)\n",
    "( utf -16의 경우 모든 파일 asscii -> utf-8 로 변환 후 읽었다. asscii자체를 utf -16으로 해석 불가능.)\n",
    "\n",
    "ASSCII를 UTF-8을 이해하는 방식으로 똑같이 이해한다는 것은 \n",
    "utf - 8 이 []1byte(8bit) charset 이여야 한다는것.\n",
    "\n",
    "하지만 utf- 8은 여러개의 byte가진 charset이어야 한다. \n",
    "( utf -8 은 8bit 고정이 아니라 8bit단위로 가변적 1~ 4 byte )\n",
    "\n",
    "*utf-8 이 1byte만 사용하는 경우.(charset이 1byte로 encdoing 경우 )\n",
    "    \n",
    "    0xxx xxxx \n",
    "\n",
    "첫 byte가 0 이면 이 charset이 1 byte로 encoding된것임.\n",
    "ASSCII와 역호환 가능한 경우 (0~127)\n",
    "\n",
    "*utf-8 이 2byte만 사용하는 경우.(charset이 2byte로 encdoing 경우 )\n",
    "(127보다 큰 경우)\n",
    "\n",
    "    110x xxxx                        10xx xxxx\n",
    "        leading byte                    continuation byte\n",
    "\n",
    "leading byte  : 110으로 시작 ---> 2byte로 encode 됬음을 알 수 있다. \n",
    "\n",
    "\n",
    "*utf-8 이 3byte만 사용하는 경우.\n",
    "\n",
    "        1110 xxxx                            10xx xxxx                            10xx xxxx\n",
    "        leading byte                    continuation byte                continuation byte  \n",
    "\n",
    "\n",
    "*utf-8 이 4byte만 사용하는 경우.\n",
    "\n",
    "        1111 0xxx                            10xx xxxx                            10xx xxxx                                    10xx xxxx\n",
    "        leading byte                    continuation byte                continuation byte                 continuation byte \n",
    "\n",
    "                                                                                                    \n",
    "\n",
    "표를 그려보겠다. \n",
    "\n",
    "1byte          :        0 ~ 127\n",
    "\n",
    "2byte          :       127  ~ 2^11\n",
    "\n",
    "3byte          :        2^11 ~ 2^16\n",
    "\n",
    "4byte          :        2^16 ~ 2^21\n",
    "\n",
    "\n",
    "\n",
    "# unicode decoding\n",
    "\n",
    "        # 1byte 경우 \n",
    "            \n",
    "                0100 0001  ----> 65\n",
    "        \n",
    "            asscii 읽듯이 단순히 2진  -----> 10진으로 변환.(역으로도 100프로 변환 가능)\n",
    "\n",
    "        # 2byte이상인 경우\n",
    "            \n",
    "                1100  0110            1001 1101\n",
    "            \n",
    "            마커 아닌 부분을 가져온다. \n",
    "            \n",
    "                    00110 011101 ----------> binary code point\n",
    "\n",
    "            여기서 2진 --->10진 변환 -----> 413 \n",
    "\n",
    "# unicode encoding\n",
    "\n",
    "    ex)\n",
    "        \n",
    "                927\n",
    "\n",
    "1.먼저 몇 byte 필요한지 ?\n",
    "    \n",
    "        127 < x < 2047\n",
    "\n",
    "        2 byte 필요하다. \n",
    "\n",
    "2.10진 -----> 2진 변환 \n",
    "\n",
    "    927 ---------> 1110 011111\n",
    "\n",
    "3.마커먼저 배열 \n",
    "\n",
    "        110x xxxx    10xx xxxx\n",
    "\n",
    "4.right contin~byte부터 체운다. \n",
    "\n",
    "        1100 1110         1001  1111\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
